{
  "cells": [
    {
      "metadata": {
        "_uuid": "aa8401d73c7a19e1a43fdd6a992ea9dcb60039a2"
      },
      "cell_type": "markdown",
      "source": "# Overview\nThe notebook shows how to extract the segmentation map for the ships, augment the images and train a simple DNN model to detect them. A few additional tweaks like balancing the ship-count out a little better have been done."
    },
    {
      "metadata": {
        "_uuid": "a6cd9d5ad61ffe3b8858769f20a5f9493f024a56"
      },
      "cell_type": "markdown",
      "source": "## Model Parameters\nWe might want to adjust these later (or do some hyperparameter optimizations)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "301a5d939c566d1487a049bb2554d09b592b18b1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "BATCH_SIZE = 2\nNB_EPOCHS = 2\n# downsampling inside the network\nNET_SCALING = None\n# downsampling in preprocessing\n#IMG_SCALING = (1, 1)\nIMG_SCALING = None\n# number of validation images to use\nVALID_IMG_COUNT = 400\n# maximum number of steps_per_epoch in training\nMAX_TRAIN_STEPS = 1500\nAUGMENT_BRIGHTNESS = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util.montage import montage2d as montage\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\nship_dir = '../input'\ntrain_image_dir = os.path.join(ship_dir, 'train')\ntest_image_dir = os.path.join(ship_dir, 'test')\nimport gc; gc.enable() # memory is tight\n\nfrom skimage.morphology import label\ndef multi_rle_encode(img):\n    labels = label(img[:, :, 0])\n    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n\n# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef masks_as_image(in_mask_list):\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros((768, 768), dtype = np.int16)\n    #if isinstance(in_mask_list, list):\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks += rle_decode(mask)\n    return np.expand_dims(all_masks, -1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ca7119188fbb4c6540d9df55f5833b55435287e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "masks = pd.read_csv(os.path.join('../input/',\n                                 'train_ship_segmentations.csv'))\nprint(masks.shape[0], 'masks found')\nprint(masks['ImageId'].value_counts().shape[0])\nmasks.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fdedd5965f47f84aa8f3aab1cad978512781a1cc"
      },
      "cell_type": "markdown",
      "source": "# Make sure encode/decode works\nGiven the process\n$$  RLE_0 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_0 \\stackrel{Encode}{\\longrightarrow} RLE_1 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_1 $$\nWe want to check if/that\n$ \\textrm{Image}_0 \\stackrel{?}{=} \\textrm{Image}_1 $\nWe could check the RLEs as well but that is more tedious. Also depending on how the objects have been labeled we might have different counts.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0081fd6f387abd7c05eb35f29575a2ee6ddc2236",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\nrle_0 = masks.query('ImageId==\"00021ddc3.jpg\"')['EncodedPixels']\nimg_0 = masks_as_image(rle_0)\nax1.imshow(img_0[:, :, 0])\nax1.set_title('Image$_0$')\nrle_1 = multi_rle_encode(img_0)\nimg_1 = masks_as_image(rle_1)\nax2.imshow(img_1[:, :, 0])\nax2.set_title('Image$_1$')\nprint('Check Decoding->Encoding',\n      'RLE_0:', len(rle_0), '->',\n      'RLE_1:', len(rle_1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "40cb72e241c0c3d8bc245b4e3c663b4a835b0011"
      },
      "cell_type": "markdown",
      "source": "# Split into training and validation groups\nWe stratify by the number of boats appearing so we have nice balances in each set"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4f008bf6898518fd371de013418f936edaa09f8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\nunique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\nunique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\nunique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n# some files are too small/corrupt\nunique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n                                                               os.stat(os.path.join(train_image_dir, \n                                                                                    c_img_id)).st_size/1024)\nunique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\nunique_img_ids['file_size_kb'].hist()\nmasks.drop(['ships'], axis=1, inplace=True)\nunique_img_ids.sample(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "871720221ac25f7f9408bfe01aeb4ccb95edbd1f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\ntrain_ids, valid_ids = train_test_split(unique_img_ids, \n                 test_size = 0.3, \n                 stratify = unique_img_ids['ships'])\ntrain_df = pd.merge(masks, train_ids)\nvalid_df = pd.merge(masks, valid_ids)\nprint(train_df.shape[0], 'training masks')\nprint(valid_df.shape[0], 'validation masks')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c21d5bff04bf9180463969ac120379345745ed03"
      },
      "cell_type": "markdown",
      "source": "### Examine Number of Ship Images\nHere we examine how often ships appear and replace the ones without any ships with 0"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2612fa47c7e9fdcaa7aa720c4e15fc86fd65d69a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df['ships'].hist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ef8115a80749ac47f295e9a70217a5553970c2b3"
      },
      "cell_type": "markdown",
      "source": "# Undersample Empty Images\nHere we undersample the empty images to get a better balanced group with more ships to try and segment"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cf0bb261eda957cb0a12a330260e1390c57c8c9",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\ndef sample_ships(in_df, base_rep_val=3000):\n    if in_df['ships'].values[0]==0:\n        return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n    else:\n        return in_df.sample(base_rep_val, replace=(in_df.shape[0]<base_rep_val))\n    \nbalanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\nbalanced_train_df['ships'].hist(bins=np.arange(10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bfa6116af44473214a67251b1e75a141e3a6777",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "len(balanced_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a3fb9fe33d81374c7bd836f5bc86a1df89190805"
      },
      "cell_type": "markdown",
      "source": "# Decode all the RLEs into Images\nWe make a generator to produce batches of images"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6181ac51577e5636995e38a9e29311cf47f513ca"
      },
      "cell_type": "code",
      "source": "def make_image_gen(in_df, batch_size = BATCH_SIZE):\n    all_batches = list(in_df.groupby('ImageId'))\n    out_rgb = []\n    out_mask = []\n    while True:\n        np.random.shuffle(all_batches)\n        for c_img_id, c_masks in all_batches:\n            rgb_path = os.path.join(train_image_dir, c_img_id)\n            c_img = imread(rgb_path)\n            c_mask = masks_as_image(c_masks['EncodedPixels'].values)\n            out_rgb += [c_img]\n            out_mask += [c_mask]\n            if len(out_rgb)>=batch_size:\n                yield np.stack(out_rgb, 0), np.stack(out_mask, 0)\n                out_rgb, out_mask=[], []",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1983738da75b031f2bec8ba36db01c095e7c5d59",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_gen = make_image_gen(balanced_train_df)\ntrain_x, train_y = next(train_gen)\nprint('x', train_x.shape, train_x.min(), train_x.max())\nprint('y', train_y.shape, train_y.min(), train_y.max())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f47639c987a10ebcb53e51f55aa8a11c98fa860"
      },
      "cell_type": "markdown",
      "source": "# Make the Validation Set"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30cb02a2a7103a9d66e90f701991199de1e5b73e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\nprint(valid_x.shape, valid_y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a8f65e7942816fb75b687a549dc1d5cc48d00e21"
      },
      "cell_type": "markdown",
      "source": "# Augment Data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.imagenet_utils import preprocess_input\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 15, \n                  width_shift_range = 0.1, \n                  height_shift_range = 0.1, \n                  shear_range = 0.01,\n                  zoom_range = [0.9, 1.25],  \n                  horizontal_flip = True, \n                  vertical_flip = True,\n                  fill_mode = 'reflect',\n                   data_format = 'channels_last',\n                  preprocessing_function=preprocess_input)\n# brightness can be problematic since it seems to change the labels differently from the images \nif AUGMENT_BRIGHTNESS:\n    dg_args[' brightness_range'] = [0.5, 1.5]\nimage_gen = ImageDataGenerator(**dg_args)\n\nif AUGMENT_BRIGHTNESS:\n    dg_args.pop('brightness_range')\ndg_args.pop('preprocessing_function')\nlabel_gen = ImageDataGenerator(**dg_args)\n\ndef create_aug_gen(in_gen, seed = None):\n    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n    for in_x, in_y in in_gen:\n        seed = np.random.choice(range(9999))\n        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n        g_x = image_gen.flow(in_x, \n                             batch_size = BATCH_SIZE, \n                             seed = seed, \n                             shuffle=True)\n        g_y = label_gen.flow(in_y, \n                             batch_size = BATCH_SIZE, \n                             seed = seed, \n                             shuffle=True)\n\n        yield next(g_x), next(g_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6122ccb9e58bfac6fa5e11c86121e78d9e5151b1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "cur_gen = create_aug_gen(train_gen)\nt_x, t_y = next(cur_gen)\nprint('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\nprint('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n# only keep first 9 samples to examine in detail\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33300c4f03b6600da7b418f775d11d7ebf76a35a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba08494eb9736ec3556b7c879143cdcdea89febf"
      },
      "cell_type": "markdown",
      "source": "# Build a Model\nHere we use a slight deviation on the U-Net standard"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10118bedfc5b3da08c44b3af8ddace7f776fffb4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.applications.vgg16 import VGG16 as VGG16, preprocess_input\nencode_model = VGG16(input_shape=(768,768,3), include_top=False, weights='imagenet')\nencode_model.trainable = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2687377309d3cbbab1197f4eccd2b50ab996f5a6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras import models, layers\n\n# output and start upsampling\nfeatures = encode_model.output\n\nconv_1 = layers.Conv2D(512, (3,3), activation='relu', padding='same')(features)\nup_conv = layers.Conv2DTranspose(256, (3,3), strides=(2,2), activation='relu', padding='same')(conv_1)\n\n# first concatenation block\nconcat_1 = layers.concatenate([encode_model.get_layer('block5_conv3').output, up_conv], axis=-1, name='concat_1')\nconv_2 = layers.Conv2D(512, (3,3), activation='relu', padding='same')(concat_1)\nup_conv_2 = layers.Conv2DTranspose(256, (3,3), strides=(2,2), activation='relu', padding='same')(conv_2)\n\n\n# second concatenation block\nconcat_2 = layers.concatenate([up_conv_2, encode_model.get_layer('block4_conv3').output])\nconv_3 = layers.Conv2D(512, (3,3), activation='relu', padding='same')(concat_2)\nup_conv_3 = layers.Conv2DTranspose(128, (3,3), strides=(2,2), activation='relu', padding='same')(conv_3)\n\n# third concatenation block\nconcat_3 = layers.concatenate([up_conv_3, encode_model.get_layer('block3_conv3').output])\nconv_4 = layers.Conv2D(256, (3,3), activation='relu', padding='same')(concat_3)\nup_conv_4 = layers.Conv2DTranspose(64, (3,3), strides=(2,2), activation='relu', padding='same')(conv_4)\n\n# fourth concatenation block\nconcat_4 = layers.concatenate([up_conv_4, encode_model.get_layer('block2_conv2').output])\nconv_5 = layers.Conv2D(128, (3,3), activation='relu', padding='same',name='block2_conv')(concat_4)\nup_conv_5 = layers.Conv2DTranspose(32, (3,3), strides=(2,2), activation='relu', padding='same')(conv_5)\n\n# fifth concatenation block\nconcat_4 = layers.concatenate([up_conv_5, encode_model.get_layer('block1_conv2').output])\nconv_6 = layers.Conv2D(1, (3,3), activation='sigmoid', padding='same')(concat_4)\n\nfor layer in encode_model.layers:\n    layer.trainable = False\n    \nfinal_model = models.Model(inputs=[encode_model.input], outputs=[conv_6])\nfinal_model.summary()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1678069aa8013510264ba898291c6ae2dce88a76",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n\ndef dice_p_bce(in_gt, in_pred):\n    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n\ndef true_positive_rate(y_true, y_pred):\n    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n\ndef IoU(y_true, y_pred, eps=1e-6):\n    if np.max(y_true) == 0.0:\n        return IoU(1-y_true, 1-y_pred) ## empty image; calc IoU of zeros\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n    return -K.mean( (intersection + eps) / (union + eps), axis=0)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7282d18de3aff1cee12ff89b7d511a391702814f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('seg_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n                                   patience=3, \n                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\nearly = EarlyStopping(monitor=\"val_dice_coef\", \n                      mode=\"max\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b67d808c0b8c7e28bff41e6d3858ff6f09dd626",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\n\ndef fit():\n    final_model.compile(optimizer=Adam(1e-3, decay=1e-6), loss=IoU, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])\n    step_count = min(MAX_TRAIN_STEPS, balanced_train_df.shape[0]//BATCH_SIZE)\n    aug_gen = create_aug_gen(make_image_gen(balanced_train_df))\n    \n    with tf.device(\"/gpu:0\"):\n        loss_history = [final_model.fit_generator(aug_gen, \n                             epochs=NB_EPOCHS, \n                            steps_per_epoch=step_count,\n                             validation_data=(valid_x, valid_y),\n                             callbacks=callbacks_list,\n                            workers=1 # the generator is not very thread safe\n                                       )]\n    return loss_history\n\ncounter = 0\nwhile True:\n    loss_history = fit()\n    counter += 1\n    # if np.min([mh.history['val_loss'] for mh in loss_history]) < -0.5 or counter==10:\n    if counter == 10:\n        break\n  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a168c8b1af446b800f6129104906003ededd61c4",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "def show_loss(loss_history):\n    epich = np.cumsum(np.concatenate(\n        [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 10))\n    _ = ax1.plot(epich,\n                 np.concatenate([mh.history['loss'] for mh in loss_history]),\n                 'b-',\n                 epich, np.concatenate(\n            [mh.history['val_loss'] for mh in loss_history]), 'r-')\n    ax1.legend(['Training', 'Validation'])\n    ax1.set_title('Loss')\n\n    _ = ax2.plot(epich, np.concatenate(\n        [mh.history['true_positive_rate'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_true_positive_rate'] for mh in loss_history]),\n                     'r-')\n    ax2.legend(['Training', 'Validation'])\n    ax2.set_title('True Positive Rate\\n(Positive Accuracy)')\n    \n    _ = ax3.plot(epich, np.concatenate(\n        [mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_binary_accuracy'] for mh in loss_history]),\n                     'r-')\n    ax3.legend(['Training', 'Validation'])\n    ax3.set_title('Binary Accuracy (%)')\n    \n    _ = ax4.plot(epich, np.concatenate(\n        [mh.history['dice_coef'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_dice_coef'] for mh in loss_history]),\n                     'r-')\n    ax4.legend(['Training', 'Validation'])\n    ax4.set_title('DICE')\n\nshow_loss(loss_history)"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ce1167e9f09200f537e61f93f486168a13be1711"
      },
      "cell_type": "code",
      "source": "final_model.load_weights(weight_path)\nfinal_model.save('segmentation_model.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "275b411dc97a350aacaba46c8562efcf2658b1a7",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "pred_y = seg_model.predict(valid_x)\nprint(pred_y.shape, pred_y.min(), pred_y.max(), pred_y.mean())"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a4fd2ca0cf47ba069a314356bf74c7b531c56ac",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\nax.hist(pred_y.ravel(), np.linspace(0, 1, 10))\nax.set_xlim(0, 1)\nax.set_yscale('log', nonposy='clip')"
    },
    {
      "metadata": {
        "_uuid": "0018ab172d18936f8cc2c5df33d2f840dc16bf4f"
      },
      "cell_type": "markdown",
      "source": "# Prepare Full Resolution Model\nHere we account for the scaling so everything can happen in the model itself"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "17408f0ee8dc16149b8eff0447a1427ab3ed82ba"
      },
      "cell_type": "markdown",
      "source": "if IMG_SCALING is not None:\n    fullres_model = models.Sequential()\n    fullres_model.add(layers.AvgPool2D(IMG_SCALING, input_shape = (None, None, 3)))\n    fullres_model.add(seg_model)\n    fullres_model.add(layers.UpSampling2D(IMG_SCALING))\nelse:\n    fullres_model = seg_model\nfullres_model.save('fullres_model.h5')"
    },
    {
      "metadata": {
        "_uuid": "17edb177402ae51651692511827a7e9d60646533"
      },
      "cell_type": "markdown",
      "source": "# Run the test data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4911811f267f9f3397a58902da9e75c6f261ad40",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "test_paths = os.listdir(test_image_dir)\nprint(len(test_paths), 'test images found')"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73ef7b3b2a74bf64968c79b4005075d4f0e23143",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "fig, m_axs = plt.subplots(20, 2, figsize = (10, 40))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor (ax1, ax2), c_img_name in zip(m_axs, test_paths):\n    c_path = os.path.join(test_image_dir, c_img_name)\n    c_img = imread(c_path)\n    first_img = np.expand_dims(c_img, 0)/255.0\n    first_seg = fullres_model.predict(first_img)\n    ax1.imshow(first_img[0])\n    ax1.set_title('Image')\n    ax2.imshow(first_seg[0, :, :, 0], vmin = 0, vmax = 1)\n    ax2.set_title('Prediction')\nfig.savefig('test_predictions.png')"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "11a6c6615131ff8c317f95a5097b46565ef21121"
      },
      "cell_type": "markdown",
      "source": "# Submission\nSince gneerating the submission takes a long time and quite a bit of memory we run it in a seperate kernel located at https://www.kaggle.com/kmader/from-trained-u-net-to-submission-part-2 \nThat kernel takes the model saved in this kernel and applies it to all the test data"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}