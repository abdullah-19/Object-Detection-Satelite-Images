{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa8401d73c7a19e1a43fdd6a992ea9dcb60039a2"
   },
   "source": [
    "# Overview\nIn this notebook I will use a U-net like architecture for an instance segmentation problem. The problem is to detect boats in satelite imagery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6cd9d5ad61ffe3b8858769f20a5f9493f024a56"
   },
   "source": [
    "## Model Parameters\nWe might want to adjust these later (or do some hyperparameter optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "301a5d939c566d1487a049bb2554d09b592b18b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "NB_EPOCHS = 2\n",
    "VALID_IMG_COUNT = 500\n",
    "DIMENSIONS = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util.montage import montage2d as montage\n",
    "import gc\n",
    "from skimage.morphology import label\n",
    "gc.enable()  # memory is tight\n",
    "%matplotlib inline\n",
    "\n",
    "ship_dir = '../input'\n",
    "train_image_dir = os.path.join(ship_dir, 'train')\n",
    "test_image_dir = os.path.join(ship_dir, 'test')\n",
    "\n",
    "\n",
    "def multi_rle_encode(img):\n",
    "    labels = label(img[:, :, 0])\n",
    "    return [rle_encode(labels == k) for k in np.unique(labels[labels > 0])]\n",
    "\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype=np.int16)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "3ca7119188fbb4c6540d9df55f5833b55435287e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masks = pd.read_csv(os.path.join(ship_dir,\n",
    "                                 'train_ship_segmentations.csv'))\n",
    "print(masks.shape[0], 'masks found')\n",
    "print('Total number of train images: {}'.format(masks['ImageId'].value_counts().shape[0]))\n",
    "masks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fdedd5965f47f84aa8f3aab1cad978512781a1cc"
   },
   "source": [
    "# Make sure encode/decode works\nGiven the process\n$$  RLE_0 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_0 \\stackrel{Encode}{\\longrightarrow} RLE_1 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_1 $$\nWe want to check if/that\n$ \\textrm{Image}_0 \\stackrel{?}{=} \\textrm{Image}_1 $\nWe could check the RLEs as well but that is more tedious. Also depending on how the objects have been labeled we might have different counts.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "0081fd6f387abd7c05eb35f29575a2ee6ddc2236",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "rle_0 = masks.query('ImageId==\"00021ddc3.jpg\"')['EncodedPixels']\n",
    "img_0 = masks_as_image(rle_0)\n",
    "ax1.imshow(img_0[:, :, 0])\n",
    "ax1.set_title('Image$_0$')\n",
    "rle_1 = multi_rle_encode(img_0)\n",
    "img_1 = masks_as_image(rle_1)\n",
    "ax2.imshow(img_1[:, :, 0])\n",
    "ax2.set_title('Image$_1$')\n",
    "print('Check Decoding->Encoding',\n",
    "      'RLE_0:', len(rle_0), '->',\n",
    "      'RLE_1:', len(rle_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40cb72e241c0c3d8bc245b4e3c663b4a835b0011"
   },
   "source": [
    "# Split into training and validation groups\nWe stratify by the number of boats appearing so we have nice balances in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "c4f008bf6898518fd371de013418f936edaa09f8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masks['numberShips'] = masks.EncodedPixels.map(lambda row: 1 if isinstance(row, str) else 0)\n",
    "\n",
    "unique_img_ids = masks.groupby('ImageId').agg({'numberShips': 'sum'}).reset_index()\n",
    "unique_img_ids['file_size_kb'] = (unique_img_ids['ImageId'].\n",
    "                                  map(lambda c_img_id: \n",
    "                                os.stat(os.path.join(train_image_dir,\n",
    "                                c_img_id)).st_size / 1024))\n",
    "unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb'] > 50]  # keep only 50kb files to get rid of truncated file\n",
    "print(unique_img_ids.shape[0])\n",
    "unique_img_ids.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "5dd5de8e64363180318ca83900340decd910160b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masks.drop('numberShips', axis=1, inplace=True)\n",
    "masks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e450360a3d8eafd9bd0d88258499732f3c3f800"
   },
   "source": [
    "## Build train and validation set according to number of boats in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "871720221ac25f7f9408bfe01aeb4ccb95edbd1f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_ids, valid_ids = train_test_split(unique_img_ids, train_size=0.7,\n",
    "                 test_size = 0.3, \n",
    "                 stratify = unique_img_ids['numberShips'],\n",
    "                                        )\n",
    "train_df = pd.merge(masks, train_ids)\n",
    "valid_df = pd.merge(masks, valid_ids)\n",
    "print(train_df.shape[0], 'training masks')\n",
    "print(valid_df.shape[0], 'validation masks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an overview of the number of ships in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "2612fa47c7e9fdcaa7aa720c4e15fc86fd65d69a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['numberShips'].hist()\n",
    "plt.show()\n",
    "valid_df['numberShips'].hist()\n",
    "train_df.head()\n",
    "print('maximum number of ships: {}'.format(train_df.numberShips.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef8115a80749ac47f295e9a70217a5553970c2b3"
   },
   "source": [
    "# Undersample Empty Images\n",
    "Here we undersample the empty images to get a better balanced group with more ships to segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "9c205a36a10b654d58ff6a4233888358196837e9"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(ratio={0: 1000, 1: 1200, 2: 1200,\n",
    "                                3: 1200, 4: 1200, 5: 1200,\n",
    "                                6: 1200, 7: 1000, 8: 1100,\n",
    "                                9: 1000, 10: 700, 11: 700,\n",
    "                                12: 600, 13: 500, 14: 500,\n",
    "                                15: 450})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "7bfa6116af44473214a67251b1e75a141e3a6777",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(train_df.ImageId).astype(np.str_).reshape(-1, 1)\n",
    "y = np.array(train_df.numberShips).astype(np.uint8)\n",
    "\n",
    "x_train, y_train = rus.fit_sample(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "557d14f451cb1b2c442a39ebcd57e5c459c570cf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_x = pd.DataFrame(x_train).rename({0: 'ImageId'}, axis=1)\n",
    "train_masks = pd.merge(balanced_x, train_df)\n",
    "balanced_x = pd.merge(balanced_x, unique_img_ids, how='left', on='ImageId')\n",
    "balanced_x.numberShips.hist(bins=15)\n",
    "train_masks = train_masks.drop(['numberShips', 'file_size_kb'], axis='columns')\n",
    "train_masks.set_index('ImageId')\n",
    "train_masks.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3fb9fe33d81374c7bd836f5bc86a1df89190805"
   },
   "source": [
    "# Decode all the RLEs into Images\n",
    "Building a generator to read in all images and masks. Also we do some resizing of the images to make the model less computational intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "6181ac51577e5636995e38a9e29311cf47f513ca"
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "def make_image_gen(in_df, batch_size = BATCH_SIZE):\n",
    "    all_batches = list(in_df.groupby('ImageId'))\n",
    "    out_rgb = []\n",
    "    out_mask = []\n",
    "    while True:\n",
    "        np.random.shuffle(all_batches)\n",
    "        for c_img_id, c_masks in all_batches:\n",
    "            rgb_path = os.path.join(train_image_dir, c_img_id)\n",
    "            c_img = imread(rgb_path)\n",
    "            c_img = resize(c_img, DIMENSIONS)\n",
    "            c_mask = masks_as_image(c_masks['EncodedPixels'].values)\n",
    "            out_rgb += [c_img]\n",
    "            out_mask += [c_mask]\n",
    "            if len(out_rgb)>=batch_size:\n",
    "                yield np.stack(out_rgb, 0), np.stack(out_mask, 0)\n",
    "                out_rgb, out_mask=[], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "8c1835a8070f2c261b9b2c220ce68dfd3f3fa095",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = make_image_gen(train_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f47639c987a10ebcb53e51f55aa8a11c98fa860"
   },
   "source": [
    "# Make the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "30cb02a2a7103a9d66e90f701991199de1e5b73e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\n",
    "print(len(valid_x), valid_x.max(), valid_x.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8f65e7942816fb75b687a549dc1d5cc48d00e21"
   },
   "source": [
    "# Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "dg_args = dict(featurewise_center=False,\n",
    "               samplewise_center=False,\n",
    "               rotation_range=15,\n",
    "               width_shift_range=0.1,\n",
    "               height_shift_range=0.1,\n",
    "               zoom_range=[0.8, 1.25],\n",
    "               horizontal_flip=True,\n",
    "               vertical_flip=True,\n",
    "               fill_mode='reflect',\n",
    "               data_format='channels_last',\n",
    "               preprocessing_function=preprocess_input)\n",
    "# brightness can be problematic since it seems to change the labels differently from the images \n",
    "\n",
    "image_gen = ImageDataGenerator(**dg_args)\n",
    "dg_args.pop('preprocessing_function')\n",
    "mask_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "def create_aug_gen(in_gen, seed=None):\n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    for in_x, in_y in in_gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n",
    "        g_x = image_gen.flow(in_x,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             seed=seed)\n",
    "        g_y = mask_gen.flow(in_y,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            seed=seed)\n",
    "\n",
    "        yield next(g_x), next(g_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "6122ccb9e58bfac6fa5e11c86121e78d9e5151b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_gen = create_aug_gen(train_gen)\n",
    "t_x, t_y = next(cur_gen)\n",
    "print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n",
    "# only keep first 9 samples to examine in detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "33300c4f03b6600da7b418f775d11d7ebf76a35a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba08494eb9736ec3556b7c879143cdcdea89febf"
   },
   "source": [
    "# Build a Model\n",
    "Here we use a slight deviation on the U-Net standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "10118bedfc5b3da08c44b3af8ddace7f776fffb4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16 as VGG16, preprocess_input\n",
    "encode_model = VGG16(input_shape=(512,512,3), include_top=False, weights='imagenet')\n",
    "encode_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "2687377309d3cbbab1197f4eccd2b50ab996f5a6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.backend import tf as ktf\n",
    "\n",
    "# output and start upsampling\n",
    "features = encode_model.output\n",
    "\n",
    "conv_1 = layers.Conv2D(512, (3,3), activation='relu', padding='same')(features)\n",
    "up_conv = layers.Conv2DTranspose(256, (3,3), strides=(2,2), activation='relu', padding='same')(conv_1)\n",
    "\n",
    "# first concatenation block\n",
    "concat_1 = layers.concatenate([encode_model.get_layer('block5_conv3').output, up_conv], axis=-1, name='concat_1')\n",
    "conv_2 = layers.Conv2D(512, (3,3), activation='relu', padding='same')(concat_1)\n",
    "up_conv_2 = layers.Conv2DTranspose(256, (3,3), strides=(2,2), activation='relu', padding='same')(conv_2)\n",
    "\n",
    "\n",
    "# second concatenation block\n",
    "concat_2 = layers.concatenate([up_conv_2, encode_model.get_layer('block4_conv3').output])\n",
    "conv_3 = layers.Conv2D(512, (3,3), activation='relu', padding='same')(concat_2)\n",
    "up_conv_3 = layers.Conv2DTranspose(128, (3,3), strides=(2,2), activation='relu', padding='same')(conv_3)\n",
    "\n",
    "# third concatenation block\n",
    "concat_3 = layers.concatenate([up_conv_3, encode_model.get_layer('block3_conv3').output])\n",
    "conv_4 = layers.Conv2D(256, (3,3), activation='relu', padding='same')(concat_3)\n",
    "up_conv_4 = layers.Conv2DTranspose(64, (3,3), strides=(2,2), activation='relu', padding='same')(conv_4)\n",
    "\n",
    "# fourth concatenation block\n",
    "concat_4 = layers.concatenate([up_conv_4, encode_model.get_layer('block2_conv2').output])\n",
    "conv_5 = layers.Conv2D(128, (3,3), activation='relu', padding='same',name='block2_conv')(concat_4)\n",
    "up_conv_5 = layers.Conv2DTranspose(32, (3,3), strides=(2,2), activation='relu', padding='same')(conv_5)\n",
    "\n",
    "# fifth concatenation block\n",
    "concat_4 = layers.concatenate([up_conv_5, encode_model.get_layer('block1_conv2').output])\n",
    "conv_6 = layers.Conv2D(1, (3,3), activation='sigmoid', padding='same')(concat_4)\n",
    "\n",
    "#scaling up again to 768, 768\n",
    "out = layers.Lambda(lambda image: ktf.image.resize_images(image, (768,768)))(conv_6)\n",
    "\n",
    "for layer in encode_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "final_model = models.Model(inputs=[encode_model.input], outputs=[out])\n",
    "final_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "1678069aa8013510264ba898291c6ae2dce88a76",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
    "\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n",
    "\n",
    "# IoU metric closer to competition metric\n",
    "def IoU(y_true, y_pred, eps=1e-6):\n",
    "    if np.max(y_true) == 0.0:\n",
    "        return IoU(1-y_true, 1-y_pred) ## empty image; calc IoU of zeros\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n",
    "    return -K.mean( (intersection + eps) / (union + eps), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "7282d18de3aff1cee12ff89b7d511a391702814f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "weight_path=\"{}_weights.best.hdf5\".format('seg_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "tensorboard = TensorBoard(\"./logs\", write_images= True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n",
    "                                   patience=3, \n",
    "                                   verbose=1, mode='min', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n",
    "#early = EarlyStopping(monitor=\"val_dice_coef\", \n",
    "                      #mode=\"max\", \n",
    "                      #patience=15) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, tensorboard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "5b67d808c0b8c7e28bff41e6d3858ff6f09dd626",
    "scrolled": false,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.compile(optimizer=Adam(1e-3, decay=1e-6), loss=IoU,\n",
    "                    metrics=[dice_coef, 'binary_accuracy',\n",
    "                             true_positive_rate])\n",
    "\n",
    "def fit():\n",
    "    step_count = int(balanced_x.shape[0] / BATCH_SIZE) / 3\n",
    "    aug_gen = create_aug_gen(make_image_gen(train_masks.sample(frac=1)))\n",
    "    loss_history = [final_model.fit_generator(aug_gen,\n",
    "                                              epochs=NB_EPOCHS,\n",
    "                                              steps_per_epoch=step_count,\n",
    "                                              validation_data=(valid_x,\n",
    "                                                               valid_y),\n",
    "                                              callbacks=callbacks_list,\n",
    "                                              workers=1)]\n",
    "    return loss_history\n",
    "\n",
    "counter = 0\n",
    "while True:\n",
    "    loss_history = fit()\n",
    "    counter += 1\n",
    "    if np.min([mh.history['val_loss'] for mh in loss_history]) < -0.5 or counter == 5:\n",
    "        # if counter == 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "ce1167e9f09200f537e61f93f486168a13be1711"
   },
   "outputs": [],
   "source": [
    "final_model.load_weights(weight_path)\n",
    "final_model.save('segmentation_model_final.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
